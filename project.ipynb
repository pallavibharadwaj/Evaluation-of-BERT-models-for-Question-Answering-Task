{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of BERT models on Question Answering Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:52:26.663674Z",
     "start_time": "2020-12-09T04:52:26.661389Z"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Question Answering is an important Natural Language Processing Task wherein a system, given a natural language question and a context document, returns the correct answer to the question. Question Answering is an on-going research domain in Natural Language Processing which has had performance breakthroughs in recent times, after the introduction of the Transformer based models.\n",
    "\n",
    "We have used the Stanford Question Answering Dataset (SQuAD) consisting of questions posed by crowdworkers based on certain Wikipedia articles. SQuAD2.0 combines questions from the SQuAD1.1 and adds unanswerable questions to the list. For a system to perform well, it has to also determine when to abstain from answering. We aim to use DistilBERT, ELECTRA, and RoBERTa, fine-tuned on the SQuAD 2.0 dataset. Ultimately, we want to provide comparisons and analysis of which models work the best and the reasoning behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "We performed a detailed analysis on the data to understand the nature of SQUAD 2.0. \n",
    "\n",
    "The [data analysis](./data_analysis.ipynb) notebook will take you through the analysis and inferences. To summarize the analysis:\n",
    "\n",
    "1. Contexts are densely present with word length of 100-150. This helps us understand how to use our models and the fine-tuning required to handle such cases. The mean length of contexts is 137.9 words and the maximum length is 766 words. The shortest context consists of 22 words.\n",
    "![](./assets/context_length.png)\n",
    "\n",
    "2. Questions comprise of 10-15 words on an average. The mean of the questions length is 11.29 words and the longest question in the dataset is 60 words long.\n",
    "![](./assets/question_length.png)\n",
    "\n",
    "3. The answers on the other hand are comparatively shorter. The following figure shows that answers mainly are are 3-4 words long. The longest answer is around 46 words long, and the shortest is just comprised of a single word.\n",
    "![](./assets/answer_length.png)\n",
    "\n",
    "4. The Objective of our models is to predict the starting index and the ending of the answers from the given context. It would be interesting to know how the indices of the starting word of answers compares to the context. The folowing graph shows that the starting indices of the answer spans occur more frequently at the beginning of the context, but have a tapering frequency as the context length keeps increasing.\n",
    "![](./assets/start_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
