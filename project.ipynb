{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of BERT models on Question Answering Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:52:26.663674Z",
     "start_time": "2020-12-09T04:52:26.661389Z"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Question Answering is an important Natural Language Processing Task wherein a system, given a natural language question and a context document, returns the correct answer to the question. Question Answering is an on-going research domain in Natural Language Processing which has had performance breakthroughs in recent times, after the introduction of the Transformer based models.\n",
    "\n",
    "We have used the Stanford Question Answering Dataset (SQuAD) consisting of questions posed by crowdworkers based on certain Wikipedia articles. SQuAD2.0 combines questions from the SQuAD1.1 and adds unanswerable questions to the list. For a system to perform well, it has to also determine when to abstain from answering. We aim to use DistilBERT, ELECTRA, and RoBERTa, fine-tuned on the SQuAD 2.0 dataset. Ultimately, we want to provide comparisons and analysis of which models work the best and the reasoning behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "We performed a detailed analysis on the data to understand the nature of SQUAD 2.0. \n",
    "\n",
    "The [data analysis](./data_analysis.ipynb) notebook will take you through the analysis and inferences. To summarize the analysis:\n",
    "\n",
    "1. Contexts are densely present with word length of 100-150. This helps us understand how to use our models and the fine-tuning required to handle such cases. The mean length of contexts is 137.9 words and the maximum length is 766 words. The shortest context consists of 22 words.\n",
    "![](./assets/context_length.png)\n",
    "\n",
    "2. Questions comprise of 10-15 words on an average. The mean of the questions length is 11.29 words and the longest question in the dataset is 60 words long.\n",
    "![](./assets/question_length.png)\n",
    "\n",
    "3. The answers on the other hand are comparatively shorter. The following figure shows that answers mainly are are 3-4 words long. The longest answer is around 46 words long, and the shortest is just comprised of a single word.\n",
    "![](./assets/answer_length.png)\n",
    "\n",
    "4. The Objective of our models is to predict the starting index and the ending of the answers from the given context. It would be interesting to know how the indices of the starting word of answers compares to the context. The folowing graph shows that the starting indices of the answer spans occur more frequently at the beginning of the context, but have a tapering frequency as the context length keeps increasing.\n",
    "![](./assets/start_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We used the pre-trained DistilBERT-base-uncased, ELECTRA-base, and RoBERTa-base networks from Hugging Face using SimpleTransformers. The library contains easy-to-use pre-trained Question-Answering BERT models of type ALBERT, BERT, DistilBERT, ELECTRA, XLM, and XLNet. The training took about 13 hours per model and per run on a 6GB NVIDIA GeForce GTX 1060 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Model Selection\n",
    "\n",
    "Model Selection is performed using a command-line argument of one of the three models during training, evaluation, checking the model performance.\n",
    "\n",
    "1. distilbert\n",
    "2. electra-base\n",
    "3. roberta\n",
    "\n",
    "For ease of comparison, we have included a shell script `run.sh` that sequentially runs all the models by removing any cache and passing the right set of arguments as following.\n",
    "\n",
    "```\n",
    "rm -r cache_dir\n",
    "python train.py distilbert\n",
    "\n",
    "rm -r cache_dir\n",
    "python train.py roberta\n",
    "\n",
    "rm -r cache_dir\n",
    "python train.py electra-base\n",
    "```\n",
    "\n",
    "The training and the evaluation scripts contain the following code to select the model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T06:05:39.589022Z",
     "start_time": "2020-12-09T06:05:39.586269Z"
    }
   },
   "outputs": [],
   "source": [
    "model_type = \"roberta\"\n",
    "\n",
    "if model_type == \"distilbert\":\n",
    "    model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "elif model_type == \"roberta\":\n",
    "    model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "elif model_type == \"electra-base\":\n",
    "    model_type = \"electra\"\n",
    "    model_name = \"deepset/electra-base-squad2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model Training\n",
    "\n",
    "\n",
    "We load the pre-trained models from Hugging Face using the SimpleTransformers library particularly for QuestionAnswering task of NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T06:09:13.423750Z",
     "start_time": "2020-12-09T06:09:13.421741Z"
    }
   },
   "outputs": [],
   "source": [
    "from simpletransformers.question_answering import QuestionAnsweringModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning**:\n",
    "We fine-tuned the pre-trained model on the SQuaD 2.0 data for two complete training epochs by tuning some of the hyper-parameters like learning rate and gradient accumulation steps. We experimented with different values and saw that we got decent results with the following hyper-parameters. We monitored the training process and stopped the model training if we saw that the loss was not improving over a short period, due to time constraints.\n",
    "\n",
    "* learning_rate (Amount by which the weights are updated during training) = 4e-5\n",
    "* adam_epsilon (The value added in Adam Optimizer to avoid division by zero) = 1e-8\n",
    "* warmup_ratio (Ratio of steps used for warm-up (very low learning rate)) = 0.06\n",
    "* max_grad_norm (Gradient clipping value used to avoid exploding gradients) = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"reprocess_input_data\": False,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"use_cached_eval_features\": True,\n",
    "    \"output_dir\": f\"./models/{model_type}\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"wandb_project\": \"QuestionAnswering Model Comparison\",\n",
    "    \"wandb_kwargs\": {\"name\": model_name},\n",
    "    \"train_batch_size\": 8,\n",
    "\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 4e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_ratio': 0.06,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "}\n",
    "\n",
    "# load the trained model\n",
    "model = QuestionAnsweringModel(model_type=model_type, \n",
    "                               model_name=f\"./models/{model_type}/\",\n",
    "                               args=train_args, \n",
    "                               use_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated model files are stored separately in their respective folders specified by `model_name=f\"./models/{model_type}/\"` in the training args.\n",
    "\n",
    "```\n",
    "├── models \n",
    "      └── distilbert\n",
    "      └── electra\n",
    "      └── roberta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Predict on Dev Data\n",
    "\n",
    "**Input**\n",
    "The input is of the following format where each paragraph contains contexts and an array of associated questions identified by Question IDs and a boolean value `is_answerable` that suggests if the question is answerable or not.\n",
    "```\n",
    "[{\n",
    "    \"title\": \"Normans\", \n",
    "    \"paragraphs\": [\n",
    "        {\n",
    "            \"context\": \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\",\n",
    "            \"qas\": [{\"question\": \"In what country is Normandy located?\", \n",
    "                    \"id\": \"56ddde6b9a695914005b9628\", \n",
    "                    \"answers\": [{\"text\": \"France\", \"answer_start\": 159}, \n",
    "                                ...],\n",
    "                    \"is_impossible\": false}]\n",
    "             ...\n",
    "         }\n",
    "   ...\n",
    "}]\n",
    "\n",
    "```\n",
    "\n",
    "We then use our fine-tuned models to predict on the dev data. The model generates top 10 answer predictions and we pick the top 1 answer for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, _ = model.predict(dev_data)\n",
    "\n",
    "# chose one of top 10 predictions\n",
    "predictions = {pred['id']: pred['answer'][0] for pred in preds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a JSON output file called `predictions.json` in the respective output folders of the model. The predictions contain the question ID asoociated with the predicted answers.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"56ddde6b9a695914005b9628\": \"France.\", \n",
    "    \"56ddde6b9a695914005b9629\": \"10th and 11th centuries\"\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "├── output\n",
    "      └── distilbert\n",
    "            └── predictions.json\n",
    "      └── electra\n",
    "            └── predictions.json\n",
    "      └── roberta\n",
    "            └── predictions.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Web Application implementation\n",
    "\n",
    "The folder structure of the application is as follows -\n",
    "\n",
    "```\n",
    "├── web_application\n",
    "      └── app.py\n",
    "      └── static|\n",
    "            └── css\n",
    "                └── home.css\n",
    "            └── js\n",
    "                └── home.js\n",
    "      └── templates\n",
    "            └── base.html\n",
    "            └── home.html\n",
    "            \n",
    "```\n",
    "\n",
    "\n",
    "After installing the requirements, the app can be run by \n",
    "```\n",
    "$ python app/app.py\n",
    "\n",
    "```\n",
    "\n",
    "The app should start at 127.0.0.1:5000/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_data function, takes in the user entered question and context as a POST request and runs our best performing model prediction on it. It returns the result in a JSON format.\n",
    "We have placed the RoBERTa model as the model that gives out the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/data', methods = ['POST'])\n",
    "def get_data():\n",
    "\tif request.method == 'POST':\n",
    "\t\tif(request.get_json() is None):\n",
    "\t\t\tdata = request.form\n",
    "\t\telse:\n",
    "\t\t\tdata = request.get_json()\n",
    "\t\tcontext = data['context']\n",
    "\t\tquestion = data['question']\n",
    "\n",
    "\t\tto_predict = [{'context': context, 'qas': [{'question':question,'id':'0'}]}]\n",
    "\n",
    "\t\tmodel_type = \"roberta\"\n",
    "\t\tmodel = QuestionAnsweringModel(model_type=model_type, \n",
    "                               model_name=f\"../models/{model_type}/\", use_cuda = False)\n",
    "\n",
    "\t\tpreds, _ = model.predict(to_predict)\n",
    "\n",
    "\t\tprint(preds[0]['answer'][0])\n",
    "\t\tif(preds[0]['answer'][0] == \"\"):\n",
    "\t\t\tresult = \"No answer found\"\n",
    "\t\telse:\n",
    "\t\t\tresult = preds[0]['answer'][0]\n",
    "\n",
    "\t\treturn jsonify({'output':result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "The evaluation is performed on the dev data using Precision, Recall, and F1 scores. The answers are first normalized by removing any stop words, punctuations, and conversion to lower case. \n",
    "\n",
    "Two evaluation metrics are computed:\n",
    "1. Raw Scores: Raw scores represent the exact count of tokens that match with one of the gold answers. This value is always an empty string for unanswerable questions.\n",
    "2. F1 Score: F1 score is computed by calculating the precision and recall over the predicted tokens. An F1 socre of 1 is given to the unanswerable questions when both predicted and gold answers are emoty strings.\n",
    "\n",
    "$$\n",
    "Predicted = \\frac{1.0 * count(tokens_{same})}{1.0 * count(tokens_{total})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Recall = \\frac{1.0 * count(tokens_{same})}{1.0 * count(tokens_{gold})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "  gold_toks = get_tokens(a_gold)\n",
    "  pred_toks = get_tokens(a_pred)\n",
    "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "  num_same = sum(common.values())\n",
    "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "    return int(gold_toks == pred_toks)\n",
    "  if num_same == 0:\n",
    "    return 0\n",
    "  precision = 1.0 * num_same / len(pred_toks)\n",
    "  recall = 1.0 * num_same / len(gold_toks)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1\n",
    "\n",
    "def get_raw_scores(dataset, preds):\n",
    "  exact_scores = {}\n",
    "  f1_scores = {}\n",
    "  for article in dataset:\n",
    "    for p in article['paragraphs']:\n",
    "      for qa in p['qas']:\n",
    "        qid = qa['id']\n",
    "        gold_answers = [a['text'] for a in qa['answers']\n",
    "                        if normalize_answer(a['text'])]\n",
    "        if not gold_answers:\n",
    "          # For unanswerable questions, only correct answer is empty string\n",
    "          gold_answers = ['']\n",
    "        if qid not in preds:\n",
    "          print('Missing prediction for %s' % qid)\n",
    "          continue\n",
    "        a_pred = preds[qid]\n",
    "        # Take max over all gold answers\n",
    "        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
    "        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
    "  return exact_scores, f1_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "\n",
    "RoBERTa is a clear winner and it is not a surprise considering its complexity, data, and pre-training approach. While we clearly notice a compute-performance trade-off with our experiments, it is important to note that DistilBERT provided fairly good results considering the model simplicity, training speed while utilizing less computational resources than the other two models. \n",
    "\n",
    "|                   |**DistilBERT** | **ELECTRA** | **RoBERTa** |\n",
    "| ----------------- | ------------- | ----------- | ----------- |\n",
    "| **HasAns_exact**  | 47.048        | 57.557      | 58.87       |\n",
    "| **HasAns_f1**     | 51.75         | 63.94       | 63.65       |\n",
    "| **NoAns_exact**   | 69.45         | 70.06       | 86.53       |\n",
    "| **NoAns_f1**      | 69.45         | 70.06       | 86.53       |\n",
    "| **exact**         | 58.27         | 63.82       | 72.72       |\n",
    "| **f1**            | 60.61         | 67.00       | 75.10       |\n",
    "\n",
    "\n",
    "* Looking at the figures below we can say that the performance of DistilBERT is fair considering the simplicity of the model, shorter runtime using much lesser computational resources.\n",
    "\n",
    "![](./assets/loss.png?raw=true \"Relative Loss\")\n",
    "\n",
    "\n",
    "\n",
    "Relative Runtimes                                                | Relative GPU Utilization\n",
    ":---------------------------------------------------------------:|:-----------------------------------------------------------------------:\n",
    "![](./assets/relative_runtime.png?raw=true \"Relative Runtimes\")  |  ![](./assets/relative_gpu_util.png?raw=true \"Relative GPU Utilization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* RoBERTa is a clear winner and it is not a surprise considering its complexity, data, and pre-training approach.\n",
    "\n",
    "* While we clearly notice a compute-performance trade-off with our experiments, it is important to note that DistilBERT provided fairly good results considering the model simplicity, training speed while utilizing less computational resources than the other two models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
