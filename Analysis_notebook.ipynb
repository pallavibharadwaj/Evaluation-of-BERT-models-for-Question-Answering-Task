{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport json\nimport sys\nimport argparse\nimport nltk\nfrom pprint import pprint\nfrom tqdm import tqdm\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir(\"/kaggle/input/processed_data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.stdout.encoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef setup_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", required=True)\n    return parser.parse_args()\n\n\ndef write_to_file(out_file, line):\n    out_file.write(line+'\\n')\n\n\ndef data_from_json(filename):\n    \"\"\"Loads JSON data from filename and returns\"\"\"\n    with open(filename) as data_file:\n        data = json.load(data_file)\n    return data\n\ndef tokenize(sequence):\n    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n    return tokens\n\ndef get_character_indices_mapping(context, context_tokens):\n    \n    acc = '' \n    current_token_idx = 0 # current word loc\n    mapping = dict()\n    \n    for char_idx, char in enumerate(context):\n        if(char != ' ' and char != '\\n'):\n            acc += char\n            context_token = context_tokens[current_token_idx] # current word token\n            if(acc == context_token):\n                start_ind = char_idx - len(acc) + 1\n                for char_loc in range(start_ind, char_idx+1):\n                    mapping[char_loc] = (acc, current_token_idx)\n                acc = ''\n                current_token_idx += 1\n               \n\n    if(current_token_idx != len(context_tokens)):\n        return None\n    else:\n        return mapping\n    \ndef total_exs(dataset):\n    \"\"\"\n    Returns the total number of (context, question, answer) triples,\n    given the data read from the SQuAD json file.\n    \"\"\"\n    total = 0\n    for article in dataset['data']:\n        for para in article['paragraphs']:\n            total += len(para['qas'])\n    return total\n\n    \ndef preprocess_and_write(dataset, devOrTrain, out_dir):\n    \n    num_exs = 0 # number of examples written to file\n    num_mappingprob, num_tokenprob, num_spanalignprob, num_impossibleqs = 0, 0, 0, 0\n    examples = []\n\n    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(devOrTrain)):\n        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n        \n        for pid in range(len(article_paragraphs)):\n            context = article_paragraphs[pid]['context'] # string\n            \n            context = context.replace(\"''\", '\" ')\n            context = context.replace(\"``\", '\" ')\n\n            context_tokens = tokenize(context) # list of strings (lowercase)\n            context = context.lower()\n            \n            qas = article_paragraphs[pid]['qas'] # list of questions\n            charloc2wordloc = get_character_indices_mapping(context, context_tokens)\n             \n            if charloc2wordloc is None: # there was a problem\n                num_mappingprob += len(qas)\n                continue # skip this context example\n\n            # for each question, process the question and answer and write to file\n            \n            for qn in qas:\n                question = qn['question'] # string\n                question_tokens = tokenize(question) \n                \n\n                # of the three answers, just take the first\n                # if it is impossible\n                if(qn[\"is_impossible\"] == True):\n                    num_impossibleqs +=1 \n                else:\n                    ans_text = (qn['answers'][0]['text']).lower()\n                    ans_start_charloc = qn['answers'][0]['answer_start'] \n                    ans_end_charloc = ans_start_charloc + len(ans_text) \n                \n                    if context[ans_start_charloc:ans_end_charloc] != ans_text:\n                        num_spanalignprob += 1\n                        continue\n                    \n                    ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n                    ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n                    assert ans_start_wordloc <= ans_end_wordloc\n                    \n                    ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n                    if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n                        num_tokenprob += 1\n                        continue # skip this question/answer pair\n\n                    examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n                    num_exs += 1\n                \n    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n    print(\"Number of (context, question, answer) triples with impossible answers: \", num_impossibleqs)\n    print(\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob + num_impossibleqs))\n\n    # shuffle examples\n    indices = list(range(len(examples)))\n    np.random.shuffle(indices)\n\n    with open(os.path.join(out_dir, devOrTrain +'.context'), 'w') as context_file,  \\\n         open(os.path.join(out_dir, devOrTrain +'.question'), 'w') as question_file,\\\n         open(os.path.join(out_dir, devOrTrain +'.answer'), 'w') as ans_text_file, \\\n         open(os.path.join(out_dir, devOrTrain +'.span'), 'w') as span_file:\n\n        for i in indices:\n            (context, question, answer, answer_span) = examples[i]\n\n            # write tokenized data to file\n            write_to_file(context_file, context)\n            write_to_file(question_file, question)\n            write_to_file(ans_text_file, answer)\n            write_to_file(span_file, answer_span)\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_folder_name= \"data\"\n\nif not os.path.exists(data_folder_name):\n    os.makedirs(data_folder_name)\n    \ntrain_filename = \"train-v2.0.json\"\ndev_filename = \"dev-v2.0.json\"\n\ntrain_data_dir = \"/kaggle/input/squad-20\"\ndev_data_dir = \"/kaggle/input/squad-dev\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = data_from_json(os.path.join(train_data_dir, train_filename))\nprint(\"Train data has %i examples total\" % total_exs(train_data))\ndev_data = data_from_json(os.path.join(dev_data_dir, dev_filename))\nprint(\"Dev data has %i examples total\" % total_exs(dev_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_and_write(dev_data, 'dev', \"data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_and_write(train_data, 'train', \"data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_context = os.path.join(\"/kaggle/input/processed-data-squad\",\"train.context\")\n\ncontext_len = []\n\nwith open(train_context) as fp:  \n    lines = fp.readlines()\n    print(len(lines))\n    for line in lines:\n        words = len(line.split())\n        context_len.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"context_len[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(color_codes=True)\nax = sns.distplot(context_len)\nax.set_title('Distribution of Context Length')\nax.set_ylabel('Density')\nplt.savefig('context_length.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncontext_array = np.array(context_len)\n\nprint(\"Min:   \", np.min(context_array))\nprint(\"Max:   \", np.max(context_array))\nprint(\"Mean:   \", np.mean(context_array))\nprint(\"25th percentile:   \", np.percentile(context_array, 25))\nprint(\"Median:            \", np.median(context_array))\nprint(\"75th percentile:   \", np.percentile(context_array, 75))\nprint(\"95th percentile:   \", np.percentile(context_array, 95))\nprint(\"99th percentile:   \", np.percentile(context_array, 99))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ques = os.path.join(\"/kaggle/input/processed-data-squad\",\"train.question\")\n\nques_len = []\ntrain_ans = os.path.join(\"/kaggle/input/processed-data-squad\",\"train.answer\")\n\nans_len = []\n\nwith open(train_ans) as fp:  \n    lines = fp.readlines()\n    print(len(lines))\n    for line in lines:\n        words = len(line.split())\n        ans_len.append(words)\nwith open(train_ques) as fp:  \n    lines = fp.readlines()\n    print(len(lines))\n    for line in lines:\n        words = len(line.split())\n        ques_len.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ques_len)\nplt.ylabel('Question Length')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(color_codes=True)\nax = sns.distplot(ques_len)\nax.set_title('Distribution of Question Length')\nax.set_ylabel('Density')\nplt.savefig('question_length.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ques_array = np.array(ques_len)\n\nprint(\"Min:   \", np.min(ques_array))\nprint(\"Max:   \", np.max(ques_array))\nprint(\"Mean:   \", np.mean(ques_array))\nprint(\"25th percentile:   \", np.percentile(ques_array, 25))\nprint(\"Median:            \", np.median(ques_array))\nprint(\"75th percentile:   \", np.percentile(ques_array, 75))\nprint(\"95th percentile:   \", np.percentile(ques_array, 95))\nprint(\"99th percentile:   \", np.percentile(ques_array, 99))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ans = os.path.join(\"/kaggle/input/processed-data-squad\",\"train.answer\")\n\nans_len = []\n\nwith open(train_ans) as fp:  \n    lines = fp.readlines()\n    print(len(lines))\n    for line in lines:\n        words = len(line.split())\n        ans_len.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ans_len)\nplt.ylabel('Answer Length')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans_array = np.array(ans_len)\n\nprint(\"Min:   \", np.min(ans_array))\nprint(\"Max:   \", np.max(ans_array))\nprint(\"Mean:   \", np.mean(ans_array))\nprint(\"25th percentile:   \", np.percentile(ans_array, 25))\nprint(\"Median:            \", np.median(ans_array))\nprint(\"75th percentile:   \", np.percentile(ans_array, 75))\nprint(\"95th percentile:   \", np.percentile(ans_array, 95))\nprint(\"99th percentile:   \", np.percentile(ans_array, 99))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(color_codes=True)\nax = sns.distplot(ans_len)\nax.set_title('Distribution of Answer Length')\nax.set_ylabel('Density')\nplt.savefig('answer_length.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ans = os.path.join(\"/kaggle/input/processed-data-squad\",\"train.span\")\n\nans_start = []\nans_end = []\n\nwith open(train_ans) as fp:  \n    lines = fp.readlines()\n    print(len(lines))\n    for line in lines:\n        words = line.split()\n        ans_start.append(int(words[0]))\n        ans_end.append(int(words[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans_start_array = np.array(ans_start)\nans_start_relative = np.true_divide(ans_start_array, context_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(color_codes=True)\nax = sns.distplot(ans_start_relative)\nax.set_title('Distribution of Answer Start Index relative to Context Length')\nax.set_ylabel('Answer Start Idx ratio')\nplt.savefig('answer_start_ratio.png')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}